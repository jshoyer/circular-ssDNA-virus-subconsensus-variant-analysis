
This code supports
a scientific manuscript
on cabbage leaf curl virus mutation
(Hoyer et al. 2022 https://doi.org/HKXQ).
It is released for transparency,
as an aid in understanding the work.
The code
has been archived
at Zenodo.org.
If someone can reuse the code
(after adjusting paths)
that is great,
but that is not the main goal here.

Input Illumina data
is available from the NCBI Sequence Read Archive
as PRJNA782339.
See 005_download-data-from-SRA.sh
for recommended file names.

graph-variant-call-results.R
reproduces Figure 3 in the paper.
It depends on work-up-variant-calls.R,
which also reproduces the supplemental tables
(along with work-up-read-counts.R).


The Slurm job scripts
are numbered in the order
in which they are to be run,
i.e. submitted with `sbatch`
(010_, 020_, 030_, ...).
The workflow is similar
to the one described
on the [[http://www.htslib.org/workflow/#mapping_to_variant][Samtools workflow page]],
but combines steps with pipes
to reduce repeated disk read/write steps.
Several parts of each script
that need to be edited
are indicated
(UPDATE_HERE).
Other parameters
(such as 'partition=main')
might need to be adjusted
on other clusters
or to run jobs that are similar
but more resource-intensive.

Indexing of virus and reference sequences
with 011 and 012 job scripts
uses the TAIR10.1 /A. thaliana/ genome assembly.
: curl -O https://www.arabidopsis.org/download_files/Genes/TAIR10_genome_release/TAIR10_chromosome_files/TAIR10_chr_all.fas.gz

Plasmid job scripts use an E. coli sequence from GenBank
to tally up bacterium-mapping reads.
If you have NCBI Entrez Direct e-utilities installed,
you can download this reference like so:
: esearch -db nucleotide -query "CP001637.1" | efetch -format fasta > CP001637.1.fasta

Indexing of reference sequences
needs to be done just once.
The other steps run in parallel,
using a Slurm job array,
with one job per library.
We trim reads with Cutadapt (010),
align them with BWA MEM (020),
and do the processing needed
for calling variants with VarScan (030).
These steps are split into multiple job scripts
mainly because the BWA MEM step
benefits more from allocating multiple CPU cores
than the others do.
Variables are defined
for the location of the input FastQ files
(FASTQDIR)
and the trimmed read FastQ files
generated from them
(CUTADAPTDIR).
`ls` is used (with `sed`)
to get prefixes from the names of those input files
that are then then used for subsequent output files.
Output from the 020 and 030 scripts
is written to a third directory
(OUTDIR).

Job scripts were run on a cluster
on which Lmod is used to manage software.
If you are not using Lmod
you'll need the relevant binaries (cutadapt, bwa, fastqc, ...)
to be in your $PATH.

Some lines in each job script (pwd, date, module list)
are included purely to provide context
in the log files
that Slurm writes to disk
(STDERR and STDOUT,
 in one or two files).


First ‘release’ of CabLCV code
(Zenodo record [[https://zenodo.org/record/5736526][5736526]])
intentionally included a non-duplicated set of job scripts
that have slightly different options commented out
and other idiosyncrasies.
snpEff job script merged into other one
for second release
